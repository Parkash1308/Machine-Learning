{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9965201,"sourceType":"datasetVersion","datasetId":6130088},{"sourceId":9965563,"sourceType":"datasetVersion","datasetId":6130344},{"sourceId":1186714,"sourceType":"datasetVersion","datasetId":674789}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"dataset='/kaggle/input/ecomercedataset/ecommerceDataset.csv'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:25.379320Z","iopub.execute_input":"2024-11-20T18:39:25.380220Z","iopub.status.idle":"2024-11-20T18:39:25.384625Z","shell.execute_reply.started":"2024-11-20T18:39:25.380185Z","shell.execute_reply":"2024-11-20T18:39:25.383472Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv(dataset, header=None, names=['Label', 'Description'])\n\nclass_counts = data['Label'].value_counts()\nprint(\"Class counts:\\n\", class_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:28.979720Z","iopub.execute_input":"2024-11-20T18:39:28.980531Z","iopub.status.idle":"2024-11-20T18:39:29.766492Z","shell.execute_reply.started":"2024-11-20T18:39:28.980496Z","shell.execute_reply":"2024-11-20T18:39:29.765260Z"}},"outputs":[{"name":"stdout","text":"Class counts:\n Label\nHousehold                 19313\nBooks                     11820\nElectronics               10621\nClothing & Accessories     8671\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print (data.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:32.974048Z","iopub.execute_input":"2024-11-20T18:39:32.974676Z","iopub.status.idle":"2024-11-20T18:39:32.983107Z","shell.execute_reply.started":"2024-11-20T18:39:32.974641Z","shell.execute_reply":"2024-11-20T18:39:32.982177Z"}},"outputs":[{"name":"stdout","text":"       Label                                        Description\n0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ...\n2  Household  SAF 'UV Textured Modern Art Print Framed' Pain...\n3  Household  SAF Flower Print Framed Painting (Synthetic, 1...\n4  Household  Incredible Gifts India Wooden Happy Birthday U...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Data Cleaning\nimport re\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:35.960759Z","iopub.execute_input":"2024-11-20T18:39:35.961597Z","iopub.status.idle":"2024-11-20T18:39:37.057416Z","shell.execute_reply.started":"2024-11-20T18:39:35.961560Z","shell.execute_reply":"2024-11-20T18:39:37.056659Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Download stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:40.103323Z","iopub.execute_input":"2024-11-20T18:39:40.103937Z","iopub.status.idle":"2024-11-20T18:39:40.186333Z","shell.execute_reply.started":"2024-11-20T18:39:40.103903Z","shell.execute_reply":"2024-11-20T18:39:40.185373Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def clean_text(text):\n    # Check if text is a string\n    if not isinstance(text, str):\n        return \"\"\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Remove stopwords\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\n# Apply the clean_text function\ndata['Description'] = data['Description'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:43.339885Z","iopub.execute_input":"2024-11-20T18:39:43.340520Z","iopub.status.idle":"2024-11-20T18:39:45.119123Z","shell.execute_reply.started":"2024-11-20T18:39:43.340485Z","shell.execute_reply":"2024-11-20T18:39:45.118168Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Perform a stratified split (70-30)\ntrain_data, test_data = train_test_split(\n    data, test_size=0.3, stratify=data['Label'], random_state=42\n)\n\n# Separate texts and labels\ntrain_texts = train_data['Description'].values\ntrain_labels = train_data['Label'].values\ntest_texts = test_data['Description'].values\ntest_labels = test_data['Label'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:48.019004Z","iopub.execute_input":"2024-11-20T18:39:48.019404Z","iopub.status.idle":"2024-11-20T18:39:48.075876Z","shell.execute_reply.started":"2024-11-20T18:39:48.019365Z","shell.execute_reply":"2024-11-20T18:39:48.075081Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\nencoder = LabelEncoder()\ntrain_labels = encoder.fit_transform(train_labels)\ntest_labels = encoder.transform(test_labels)\n\n# Convert to categorical format\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:39:52.065464Z","iopub.execute_input":"2024-11-20T18:39:52.066345Z","iopub.status.idle":"2024-11-20T18:39:52.082858Z","shell.execute_reply.started":"2024-11-20T18:39:52.066308Z","shell.execute_reply":"2024-11-20T18:39:52.082056Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 1 TextVectorization with one-gram multi_hot encoding\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\nvectorizer = TextVectorization(max_tokens=10000, output_mode='multi_hot', ngrams=1)\nvectorizer.adapt(train_texts)\n\nmodel = Sequential([\n    vectorizer,\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model with 10% validation split\nhistory = model.fit(train_texts, train_labels, epochs=5, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:49:58.703639Z","iopub.execute_input":"2024-11-20T17:49:58.704214Z","iopub.status.idle":"2024-11-20T17:50:28.449475Z","shell.execute_reply.started":"2024-11-20T17:49:58.704174Z","shell.execute_reply":"2024-11-20T17:50:28.448722Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8692 - loss: 0.4647 - val_accuracy: 0.9578 - val_loss: 0.1696\nEpoch 2/5\n\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0679 - val_accuracy: 0.9640 - val_loss: 0.1549\nEpoch 3/5\n\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9938 - loss: 0.0277 - val_accuracy: 0.9649 - val_loss: 0.1713\nEpoch 4/5\n\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0153 - val_accuracy: 0.9694 - val_loss: 0.1779\nEpoch 5/5\n\u001b[1m993/993\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0107 - val_accuracy: 0.9640 - val_loss: 0.1999\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:50:32.959052Z","iopub.execute_input":"2024-11-20T17:50:32.959383Z","iopub.status.idle":"2024-11-20T17:50:33.591478Z","shell.execute_reply.started":"2024-11-20T17:50:32.959355Z","shell.execute_reply":"2024-11-20T17:50:33.590550Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n              precision    recall  f1-score      support\nClass 1        0.987995  0.996368  0.992164   826.000000\nClass 2        1.000000  0.993232  0.996604   591.000000\nClass 3        0.993532  0.994819  0.994175   772.000000\nClass 4        0.995512  0.992543  0.994025  1341.000000\naccuracy       0.994051  0.994051  0.994051     0.994051\nmacro avg      0.994260  0.994240  0.994242  3530.000000\nweighted avg   0.994072  0.994051  0.994054  3530.000000\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 2. TextVectorization with two-gram multi_hot encoding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\n\nvectorizer = TextVectorization(max_tokens=10000, output_mode='multi_hot', ngrams=2)\nvectorizer.adapt(train_texts)\n\nmodel = Sequential([\n    vectorizer,\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_texts, train_labels, epochs=6, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:50:44.647468Z","iopub.execute_input":"2024-11-20T17:50:44.647813Z","iopub.status.idle":"2024-11-20T17:51:17.568512Z","shell.execute_reply.started":"2024-11-20T17:50:44.647783Z","shell.execute_reply":"2024-11-20T17:51:17.567803Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.8871 - loss: 0.4882 - val_accuracy: 0.9669 - val_loss: 0.1300\nEpoch 2/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0712 - val_accuracy: 0.9704 - val_loss: 0.1087\nEpoch 3/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9922 - loss: 0.0308 - val_accuracy: 0.9748 - val_loss: 0.1156\nEpoch 4/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0162 - val_accuracy: 0.9745 - val_loss: 0.1288\nEpoch 5/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0130 - val_accuracy: 0.9751 - val_loss: 0.1431\nEpoch 6/6\n\u001b[1m894/894\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0096 - val_accuracy: 0.9748 - val_loss: 0.1420\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:51:19.992760Z","iopub.execute_input":"2024-11-20T17:51:19.993591Z","iopub.status.idle":"2024-11-20T17:51:20.594145Z","shell.execute_reply.started":"2024-11-20T17:51:19.993558Z","shell.execute_reply":"2024-11-20T17:51:20.593286Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n              precision    recall  f1-score      support\nClass 1        0.995896  0.994536  0.995215   732.000000\nClass 2        0.998172  0.998172  0.998172   547.000000\nClass 3        0.997006  0.995516  0.996260   669.000000\nClass 4        0.996751  0.998373  0.997561  1229.000000\naccuracy       0.996852  0.996852  0.996852     0.996852\nmacro avg      0.996956  0.996649  0.996802  3177.000000\nweighted avg   0.996852  0.996852  0.996852  3177.000000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#3. with two-gram tf_idf encoding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\nvectorizer = TextVectorization(max_tokens=10000, output_mode='tf_idf', ngrams=2)\nvectorizer.adapt(train_texts)\n\n\nmodel = Sequential([\n    vectorizer,\n    Dense(32, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nhistory = model.fit(train_texts, train_labels, epochs=5, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:51:32.705302Z","iopub.execute_input":"2024-11-20T17:51:32.705973Z","iopub.status.idle":"2024-11-20T17:52:38.308317Z","shell.execute_reply.started":"2024-11-20T17:51:32.705928Z","shell.execute_reply":"2024-11-20T17:52:38.307593Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8515 - loss: 0.6661 - val_accuracy: 0.9619 - val_loss: 0.2129\nEpoch 2/5\n\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9800 - loss: 0.0951 - val_accuracy: 0.9671 - val_loss: 0.1884\nEpoch 3/5\n\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9890 - loss: 0.0526 - val_accuracy: 0.9689 - val_loss: 0.2421\nEpoch 4/5\n\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0312 - val_accuracy: 0.9678 - val_loss: 0.2091\nEpoch 5/5\n\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9936 - loss: 0.0251 - val_accuracy: 0.9717 - val_loss: 0.2211\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:52:56.028406Z","iopub.execute_input":"2024-11-20T17:52:56.028774Z","iopub.status.idle":"2024-11-20T17:52:56.622920Z","shell.execute_reply.started":"2024-11-20T17:52:56.028743Z","shell.execute_reply":"2024-11-20T17:52:56.622078Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n              precision    recall  f1-score      support\nClass 1        0.993701  0.996840  0.995268   633.000000\nClass 2        1.000000  1.000000  1.000000   463.000000\nClass 3        0.992212  0.990669  0.991440   643.000000\nClass 4        0.994638  0.993750  0.994194  1120.000000\naccuracy       0.994753  0.994753  0.994753     0.994753\nmacro avg      0.995138  0.995315  0.995225  2859.000000\nweighted avg   0.994753  0.994753  0.994753  2859.000000\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 4. with max length 200, max tokens = 10000, and output mode = 'int'\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\nvectorizer = TextVectorization(\n    max_tokens=10000,\n    output_mode='int',\n    output_sequence_length=200\n)\n\nvectorizer.adapt(train_texts)\n\nmodel = Sequential([\n    vectorizer,\n    Embedding(input_dim=10000, output_dim=32),\n    LSTM(32),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nhistory = model.fit(train_texts, train_labels, epochs=8, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:53:25.371123Z","iopub.execute_input":"2024-11-20T17:53:25.371849Z","iopub.status.idle":"2024-11-20T17:54:30.981615Z","shell.execute_reply.started":"2024-11-20T17:53:25.371818Z","shell.execute_reply":"2024-11-20T17:54:30.980945Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.4243 - loss: 1.2872 - val_accuracy: 0.5136 - val_loss: 1.0915\nEpoch 2/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.5324 - loss: 1.0832 - val_accuracy: 0.3986 - val_loss: 1.3154\nEpoch 3/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.4070 - loss: 1.3045 - val_accuracy: 0.5451 - val_loss: 1.0417\nEpoch 4/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.6208 - loss: 0.9182 - val_accuracy: 0.7339 - val_loss: 0.6597\nEpoch 5/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.7986 - loss: 0.5724 - val_accuracy: 0.8815 - val_loss: 0.4167\nEpoch 6/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9233 - loss: 0.2961 - val_accuracy: 0.9324 - val_loss: 0.2827\nEpoch 7/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9426 - loss: 0.2359 - val_accuracy: 0.9437 - val_loss: 0.2281\nEpoch 8/8\n\u001b[1m724/724\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9619 - loss: 0.1642 - val_accuracy: 0.9468 - val_loss: 0.2231\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:54:34.146290Z","iopub.execute_input":"2024-11-20T17:54:34.146647Z","iopub.status.idle":"2024-11-20T17:54:34.950707Z","shell.execute_reply.started":"2024-11-20T17:54:34.146616Z","shell.execute_reply":"2024-11-20T17:54:34.949941Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n              precision    recall  f1-score      support\nClass 1        0.968852  0.970443  0.969647   609.000000\nClass 2        0.974239  0.951945  0.962963   437.000000\nClass 3        0.879004  0.974359  0.924228   507.000000\nClass 4        0.986667  0.942214  0.963928  1021.000000\naccuracy       0.956876  0.956876  0.956876     0.956876\nmacro avg      0.952190  0.959740  0.955192  2574.000000\nweighted avg   0.959136  0.956876  0.957298  2574.000000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 5. using GloVe\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\nvectorizer = TextVectorization(\n    max_tokens=10000,\n    output_mode='int',\n    output_sequence_length=200\n)\n\nvectorizer.adapt(train_texts)\n\ndef load_glove_embeddings(glove_file, embedding_dim=100):\n    embeddings_index = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    return embeddings_index\n\nglove_file = '/kaggle/input/glovedataset/glove.6B.100d.txt'\nembedding_dim = 100\n\nembeddings_index = load_glove_embeddings(glove_file, embedding_dim)\n\nembedding_matrix = np.zeros((10000, embedding_dim))\nfor i in range(10000):\n    word = vectorizer.get_vocabulary()[i]\n    if word in embeddings_index:\n        embedding_matrix[i] = embeddings_index[word]\n\nmodel = Sequential([\n    vectorizer,\n    Embedding(input_dim=10000, output_dim=embedding_dim,\n              weights=[embedding_matrix], input_length=200, trainable=False),\n    LSTM(32),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_texts, train_labels, epochs=5, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:27:53.992868Z","iopub.execute_input":"2024-11-20T18:27:53.993264Z","iopub.status.idle":"2024-11-20T18:32:06.599159Z","shell.execute_reply.started":"2024-11-20T18:27:53.993234Z","shell.execute_reply":"2024-11-20T18:32:06.598304Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.5038 - loss: 1.1608 - val_accuracy: 0.5898 - val_loss: 0.9516\nEpoch 2/5\n\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5950 - loss: 0.9889 - val_accuracy: 0.6274 - val_loss: 0.8823\nEpoch 3/5\n\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6161 - loss: 0.9415 - val_accuracy: 0.3985 - val_loss: 1.3119\nEpoch 4/5\n\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.4145 - loss: 1.2933 - val_accuracy: 0.4629 - val_loss: 1.2099\nEpoch 5/5\n\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.4774 - loss: 1.1747 - val_accuracy: 0.4028 - val_loss: 1.2908\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:32:09.083321Z","iopub.execute_input":"2024-11-20T18:32:09.083649Z","iopub.status.idle":"2024-11-20T18:32:09.870943Z","shell.execute_reply.started":"2024-11-20T18:32:09.083622Z","shell.execute_reply":"2024-11-20T18:32:09.870115Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n              precision    recall  f1-score      support\nClass 1        0.754386  0.078755  0.142620   546.000000\nClass 2        0.562500  0.042353  0.078775   425.000000\nClass 3        0.596774  0.078390  0.138577   472.000000\nClass 4        0.392610  0.973654  0.559579   873.000000\naccuracy       0.409326  0.409326  0.409326     0.409326\nmacro avg      0.576567  0.293288  0.229888  2316.000000\nweighted avg   0.550683  0.409326  0.287250  2316.000000\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":" #6 with FastText\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, TextVectorization\nfrom tensorflow.keras.optimizers import Adam\n\nvectorizer = TextVectorization(\n    max_tokens=10000,\n    output_mode='int',\n    output_sequence_length=200\n)\n\nvectorizer.adapt(train_texts)\n\ndef load_fasttext_embeddings(fasttext_file, embedding_dim=100):\n    embeddings_index = {}\n    with open(fasttext_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.rstrip().split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    return embeddings_index\n\nfasttext_file = '/kaggle/input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\nembedding_dim = 300\n\nembeddings_index = load_fasttext_embeddings(fasttext_file, embedding_dim)\n\nembedding_matrix = np.zeros((10000, embedding_dim))\nfor i in range(10000):\n    word = vectorizer.get_vocabulary()[i]\n    if word in embeddings_index:\n        embedding_matrix[i] = embeddings_index[word]\n\nmodel = Sequential([\n    vectorizer,\n    Embedding(input_dim=10000, output_dim=embedding_dim,\n              weights=[embedding_matrix], input_length=200, trainable=False),\n    LSTM(32),\n    Dense(16, activation='relu'),\n    Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_texts, train_labels, epochs=5, validation_split=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T18:40:09.027897Z","iopub.execute_input":"2024-11-20T18:40:09.028626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)\n\nval_predictions = model.predict(val_texts)\n\n# Convert predictions to the class with the highest probability\nval_predictions = val_predictions.argmax(axis=1)\n\n# Get the true labels from the validation set (convert from one-hot encoding to class indices)\nval_true_labels = val_labels.argmax(axis=1)\n\nreport = classification_report(val_true_labels, val_predictions, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4'], output_dict=True)\n\n\nreport_df = pd.DataFrame(report).transpose()\n\nprint(report_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}